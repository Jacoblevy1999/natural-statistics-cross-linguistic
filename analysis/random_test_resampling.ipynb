{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "british-cuisine",
   "metadata": {},
   "source": [
    "# Natural Statistics Cross-linguistic: \n",
    "\n",
    "#### Random test resampling\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-hospital",
   "metadata": {},
   "source": [
    "##### Generate word corpora .csv files across languages \n",
    "\n",
    "Each individual word in the language corpus down the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "objective-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"data_proc\")\n",
    "import ranges_proc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fluid-breakdown",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Psuedocode\n",
    "#\n",
    "\n",
    "## loop through each language\n",
    "\n",
    "## loop through each utterance\n",
    "\n",
    "## split the utterance into words\n",
    "\n",
    "## loop through the split up words\n",
    "\n",
    "## append each to their appropriate language sample\n",
    "\n",
    "## save each language sample as their own .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "oriental-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dat_inc = pd.read_csv(\"../data/rand_dat_inc_master.csv\",index_col=0,low_memory=False)\n",
    "rand_dat_inc=rand_dat_inc[rand_dat_inc[\"language\"]!=\"ara\"]\n",
    "rand_dat_inc=rand_dat_inc[(rand_dat_inc[\"target_child_age\"]>=5) & (rand_dat_inc[\"target_child_age\"]<=30)]\n",
    "\n",
    "rand_dat_inc_cg = rand_dat_inc[rand_dat_inc[\"caregiver\"]==\"caregiver\"]\n",
    "\n",
    "rand_dat_inc_cg[\"contingent\"] = np.where(rand_dat_inc_cg[\"contingent\"]==1, \"contingent\", \"non-contingent\")\n",
    "\n",
    "rand_dat_inc_cg = rand_dat_inc_cg[rand_dat_inc_cg[\"gloss\"].notna()]\n",
    "rand_dat_inc_cg = rand_dat_inc_cg[rand_dat_inc_cg[\"gloss\"]!=\"xxx\"]\n",
    "rand_dat_inc_cg = rand_dat_inc_cg[rand_dat_inc_cg[\"gloss\"]!=\"yyy\"]\n",
    "rand_dat_inc_cg = rand_dat_inc_cg[rand_dat_inc_cg[\"gloss\"]!=\"www\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yellow-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_dat_inc_cg_cc = rand_dat_inc_cg[rand_dat_inc_cg[\"contingent\"]==\"contingent\"]\n",
    "rand_dat_inc_cg_nc = rand_dat_inc_cg[rand_dat_inc_cg[\"contingent\"]==\"non-contingent\"]\n",
    "langs = [\"German\",\"English\",\"Estonian\",\"Persian\",\"French\",\"Croatian\",\"Japanese\",\"Korean\",\"Norwegian\",\"Polish\",\"Portuguese\",\"Spanish\",\"Swedish\",\"Mandarin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generic-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # contingent word corpus\n",
    "\n",
    "# outpath = \"../data/contingent_random_test_sample_\"\n",
    "\n",
    "# for l in langs:\n",
    "#     lang = l\n",
    "#     test_samp = pd.DataFrame(columns=[\"words\"])\n",
    "#     lang_frame = rand_dat_inc_cg_cc[rand_dat_inc_cg_cc[\"Language_name\"]==lang]\n",
    "#     for index, row in lang_frame.iterrows():\n",
    "#         utterance = str(row[\"gloss\"])\n",
    "#         split = utterance.split()\n",
    "#         for word in split:\n",
    "#             test_samp = test_samp.append({'words': word}, ignore_index=True)\n",
    "#     test_samp.to_csv(\"%s%s.csv\" % (outpath,lang))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "committed-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # non-contingent word corpus\n",
    "\n",
    "# outpath = \"../data/non-contingent_random_test_sample_\"\n",
    "\n",
    "# for l in langs:\n",
    "#     lang = l\n",
    "#     print(lang)\n",
    "#     test_samp = pd.DataFrame(columns=[\"words\"])\n",
    "#     lang_frame = rand_dat_inc_cg_nc[rand_dat_inc_cg_nc[\"Language_name\"]==lang]\n",
    "#     for index, row in lang_frame.iterrows():\n",
    "#         utterance = str(row[\"gloss\"])\n",
    "#         split = utterance.split()\n",
    "#         for word in split:\n",
    "#             test_samp = test_samp.append({'words': word}, ignore_index=True)\n",
    "#     test_samp.to_csv(\"%s%s.csv\" % (outpath,lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-employment",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "##### Generated random test samples \n",
    "\n",
    "Loop through the corpora and randomly sample contingent and contingent words at increasing sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advised-gazette",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # random test samples\n",
    "\n",
    "# c_inpath = \"../data/contingent_random_test_sample_\"\n",
    "# nc_inpath = \"../data/non-contingent_random_test_sample_\"\n",
    "\n",
    "# columns = ['100','200','300','400','500','600','700','800','900','1000','1100','1200','1300','1400','1500','1600','1700','1800','1900','2000','2100','2200','2300','2400','2500','2600','2700','2800','2900','3000']\n",
    "# sample_output_c = pd.DataFrame(index=range(1,101),columns=columns)\n",
    "# sample_output_nc = pd.DataFrame(index=range(1,101),columns=columns)\n",
    "\n",
    "# for l in langs:\n",
    "#     lang = l\n",
    "#     print(lang)\n",
    "#     contingent_control_data = pd.read_csv(\"%s%s.csv\" % (c_inpath,lang),index_col=0)\n",
    "#     noncontingent_control_data = pd.read_csv(\"%s%s.csv\" % (nc_inpath,lang),index_col=0)\n",
    "#     for a in range(100,3100,100):\n",
    "#         print(a)\n",
    "#         for b in range(0,100):\n",
    "#             sample_c = []\n",
    "#             sample_nc = []\n",
    "#             for c in range(0,a):\n",
    "#                 sample = contingent_control_data.loc[np.random.permutation(contingent_control_data. index)[1]]\n",
    "#                 for y in sample:\n",
    "#                     sample_c.append(y)\n",
    "#                 sample = noncontingent_control_data.loc[np.random.permutation(noncontingent_control_data. index)[1]]\n",
    "#                 for y in sample:\n",
    "#                     sample_nc.append(y)\n",
    "#             counter = 1\n",
    "#     #       contingent\n",
    "#             word_mat = pd.DataFrame()\n",
    "#             columns = ['condition','word','word#','count']\n",
    "#             word_mat = pd.DataFrame(columns=columns)\n",
    "#             for d in range(len(sample_c)):\n",
    "#                 if counter == 1:\n",
    "#                     word_mat.loc[counter,'condition']='contingent'\n",
    "#                     word_mat.loc[counter,'word']=sample_c[d]\n",
    "#                     word_mat.loc[counter,'count']=1\n",
    "#                     word_mat.loc[counter,'word#']=counter\n",
    "#                     counter += 1\n",
    "#                 elif counter > 1:\n",
    "#                     eq_test = sample_c[d] == word_mat['word']\n",
    "#                     if np.sum(eq_test) > 0:\n",
    "#                         rep_ind = [i for i, x in enumerate(eq_test) if x]\n",
    "#                         rep_ind = str(rep_ind)\n",
    "#                         rep_ind = [ g for g in rep_ind if g.isnumeric() ]\n",
    "#                         rep_ind = ''.join(rep_ind)\n",
    "#                         word_mat.loc[int(rep_ind)+1,'count']+=1\n",
    "#                     else:\n",
    "#                         word_mat.loc[counter,'condition']='contingent'\n",
    "#                         word_mat.loc[counter,'word']=sample_c[d]\n",
    "#                         word_mat.loc[counter,'count']=1\n",
    "#                         word_mat.loc[counter,'word#']=counter\n",
    "#                         counter += 1\n",
    "#             sample_output_c.loc[b+1,\"%s\" % a,] = len(word_mat)\n",
    "#     #       non-contingent\n",
    "#             word_mat = pd.DataFrame()\n",
    "#             columns = ['condition','word','word#','count']\n",
    "#             word_mat = pd.DataFrame(columns=columns)\n",
    "#             counter = 1\n",
    "#             for f in range(len(sample_nc)):\n",
    "#                 if counter == 1:\n",
    "#                     word_mat.loc[counter,'condition']='non-contingent'\n",
    "#                     word_mat.loc[counter,'word']=sample_nc[f]\n",
    "#                     word_mat.loc[counter,'count']=1\n",
    "#                     word_mat.loc[counter,'word#']=counter\n",
    "#                     counter += 1\n",
    "#                 elif counter > 1:\n",
    "#                     eq_test = sample_nc[f] == word_mat['word']\n",
    "#                     if np.sum(eq_test) > 0:\n",
    "#                         rep_ind = [i for i, x in enumerate(eq_test) if x]\n",
    "#                         rep_ind = str(rep_ind)\n",
    "#                         rep_ind = [ g for g in rep_ind if g.isnumeric() ]\n",
    "#                         rep_ind = ''.join(rep_ind)\n",
    "#                         word_mat.loc[int(rep_ind)+1,'count']+=1\n",
    "#                     else:\n",
    "#                         word_mat.loc[counter,'condition']='non-contingent'\n",
    "#                         word_mat.loc[counter,'word']=sample_nc[f]\n",
    "#                         word_mat.loc[counter,'count']=1\n",
    "#                         word_mat.loc[counter,'word#']=counter\n",
    "#                         counter += 1\n",
    "#             sample_output_nc.loc[b+1,\"%s\" % a] = len(word_mat)\n",
    "#     sample_output_c.to_csv('../data/c_control_random_sample_%s.csv' % (lang))\n",
    "#     sample_output_nc.to_csv('../data/nc_control_random_sample_%s.csv' % (lang))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-quality",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "##### Plots & Descriptives\n",
    "\n",
    "Plot type token ratio curves and observe size at which ranges are completely nonoverlapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "threaded-consciousness",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Psuedocode\n",
    "#\n",
    "\n",
    "# import all data files\n",
    "\n",
    "# generate dataframe of averages, maxes and mins across languages.\n",
    "\n",
    "# detect when ranges are non-overlapping and save that value across languages\n",
    "\n",
    "# load R\n",
    "\n",
    "# facet scatter plot of averages and dashed line where non-overlap is achieved\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "unlimited-chocolate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load files and format structure\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "dfs = []\n",
    "\n",
    "path = \"../data/test_samples/\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "\n",
    "df_names = [elem.split(\"/\")[-1] for elem in all_files]\n",
    "df_names = [elem.replace(\".csv\", '') for elem in df_names]\n",
    "\n",
    "for f in all_files:\n",
    "    df = pd.read_csv(f,index_col=0)\n",
    "    df['filename'] = f\n",
    "    dfs.append(df)\n",
    "\n",
    "test_samples = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "test_samples[\"filename\"] = test_samples[\"filename\"].str.replace(\".csv\", '')\n",
    "test_samples[\"filename\"] = test_samples[\"filename\"].str.split(\"/\").str.get(3)\n",
    "\n",
    "test_samples = test_samples.rename(columns={\"filename\": \"corpus\"})\n",
    "\n",
    "test_samples[\"contingency\"] = test_samples[\"corpus\"].str.split(\"_\").str.get(0)\n",
    "test_samples[\"language\"] = test_samples[\"corpus\"].str.split(\"_\").str.get(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interim-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample means\n",
    "\n",
    "iter_list = test_samples.loc[: , test_samples.columns != 'corpus'].columns.tolist()\n",
    "\n",
    "test_samples1 = test_samples[(test_samples[\"language\"] == \"English\") |\n",
    "                             (test_samples[\"language\"] == \"French\") |\n",
    "                             (test_samples[\"language\"] == \"German\") |\n",
    "                             (test_samples[\"language\"] == \"Spanish\")]\n",
    "\n",
    "test_samples2 = test_samples[(test_samples[\"language\"] != \"English\") &\n",
    "                             (test_samples[\"language\"] != \"French\") &\n",
    "                             (test_samples[\"language\"] != \"German\") &\n",
    "                             (test_samples[\"language\"] != \"Spanish\")]\n",
    "\n",
    "test_samples2=test_samples2.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25d14067-f853-4eb6-97e4-2f80a086cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples1_grouped = test_samples1.groupby([\"contingency\",\"language\"])\n",
    "test_samples2_grouped = test_samples2.groupby([\"contingency\",\"language\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4290b957-9722-450b-b3e4-9497c469352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_grouped = test_samples.groupby([\"contingency\",\"language\"])\n",
    "\n",
    "test_samples_sum_stats = (test_samples_grouped.agg([\"mean\"])\n",
    "                         .reset_index())\n",
    "\n",
    "test_samples_sum_stats.columns = test_samples_sum_stats.columns.get_level_values(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604296f9-3e67-4687-a65e-f8d0ba165a60",
   "metadata": {},
   "source": [
    "----\n",
    "Find non-overlapping ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea69f3f-4488-4786-ae68-898450757c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_samples_rangov_stats1 = ranges_proc.extract_nonov_ranges(test_samples1_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c7c40cf-7f3a-4dec-bd5e-3e7d0865175f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_rangov_stats2 = ranges_proc.extract_nonov_ranges(test_samples2_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6281a2a-62a1-4fa2-8bc5-bdd8482f77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_rangov_stats1=test_samples_rangov_stats1.fillna('')\n",
    "test_samples_rangov_stats2=test_samples_rangov_stats2.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296f2e1b-b6c3-4f19-a7b7-d644e419e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_rangov_stats['language'] = test_samples_rangov_stats['language'].astype(str)\n",
    "test_samples_rangov_stats['nonov_range'] = pd.to_numeric(test_samples_rangov_stats['nonov_range'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b698ce5b-166b-47ee-afaa-c20bfbc2d232",
   "metadata": {},
   "source": [
    "For those with non-overlapping ranges, what is the ratio of the number of words we needed to sample through to the total number of words in their sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7c86096-7786-44dd-b2a5-b3f27d67524e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cword_total_count = pd.DataFrame(columns=[\"language\",\"total_word_count\"])\n",
    "\n",
    "outpath = \"../data/contingent_random_test_sample_\"\n",
    "\n",
    "for l in langs:\n",
    "    lang = l\n",
    "    lang_dat = pd.read_csv(\"%s%s.csv\" % (outpath,lang))\n",
    "    count = len(lang_dat)\n",
    "    cword_total_count = cword_total_count.append({'language' : lang,\n",
    "                                                  'total_word_count' : count} ,\n",
    "                                                  ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73879a1d-ee21-412b-90c5-39ee80a996a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncword_total_count = pd.DataFrame(columns=[\"language\",\"total_word_count\"])\n",
    "\n",
    "outpath = \"../data/non-contingent_random_test_sample_\"\n",
    "\n",
    "for l in langs:\n",
    "    lang = l\n",
    "    lang_dat = pd.read_csv(\"%s%s.csv\" % (outpath,lang))\n",
    "    count = len(lang_dat)\n",
    "    ncword_total_count = ncword_total_count.append({'language' : lang,\n",
    "                                                  'total_word_count' : count} ,\n",
    "                                                  ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f9afd1-391d-4b7a-a152-3e904f4fd902",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_total_count = pd.DataFrame(columns=[\"language\",\n",
    "                                         \"ctotal_word_count\",\n",
    "                                         \"nctotal_word_count\"])\n",
    "\n",
    "word_total_count[\"language\"] = cword_total_count[\"language\"]\n",
    "word_total_count[\"ctotal_word_count\"] = cword_total_count[\"total_word_count\"]\n",
    "word_total_count[\"nctotal_word_count\"] = ncword_total_count[\"total_word_count\"]\n",
    "word_total_count[\"total_word_count\"] = word_total_count[\"ctotal_word_count\"] + word_total_count[\"nctotal_word_count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce704dd0-83aa-45fd-8d52-cad59793f266",
   "metadata": {},
   "source": [
    "----\n",
    "Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "certified-rainbow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wide to long\n",
    "\n",
    "test_samples_sum_stats = pd.melt(test_samples_sum_stats, id_vars=[\"contingency\",\"language\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "380b508a-3bce-48e7-9576-2ff64cee6edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_sum_stats=test_samples_sum_stats.merge(test_samples_rangov_stats,on='language',how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2db33da5-09d0-4112-a997-ffa3c6b01f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_sum_stats=test_samples_sum_stats.merge(word_total_count,on='language',how=\"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b1be164-b8d1-4526-a265-d189c1a680fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_sum_stats[\"prop_samp_to_nonov\"] = (test_samples_sum_stats[\"nonov_range\"] / test_samples_sum_stats[\"total_word_count\"]).fillna(0)\n",
    "\n",
    "test_samples_sum_stats[\"prop_samp_to_nonov\"] = test_samples_sum_stats[\"prop_samp_to_nonov\"].round(decimals = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08109d83-186d-487c-a75c-be81196761e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_nonov_range = test_samples_sum_stats.drop_duplicates(['language'])\n",
    "prop_nonov_range = prop_nonov_range[[\"language\",\"prop_samp_to_nonov\"]].reset_index(drop=True)\n",
    "prop_nonov_range.replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-morrison",
   "metadata": {},
   "source": [
    "Simple plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "about-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "following-federal",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%R -i test_samples_sum_stats\n",
    "\n",
    "library('ggplot2')\n",
    "library('repr')\n",
    "options(repr.plot.width=8.27, repr.plot.height=11.69, repr.plot.res = 1200)\n",
    "\n",
    "level_order = c(100,200,300,400,500,600,700,800,900,1000,1100,1200,1300,1400,1500,1600,1700,1800,1900,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000)\n",
    "\n",
    "p <- ggplot(test_samples_sum_stats, aes(x = factor(variable,level=level_order),\n",
    "                                        y = value,\n",
    "                                        color = language)) +\n",
    "     geom_point(shape=21,\n",
    "                aes(fill = contingency),\n",
    "                size=2) +\n",
    "#      facet_wrap(. ~ language,ncol = 2) + \n",
    "     facet_wrap(. ~ language,ncol = 7) + \n",
    "     scale_fill_manual(values = alpha(c(\"black\", \"white\"), .3)) +\n",
    "     scale_x_discrete(breaks = seq(0, 3000, by = 500)) +\n",
    "     ylim(0, 1500) +\n",
    "     labs(y = \"Types (Unique Words)\", x = \"Tokens (Total Words)\") +\n",
    "     theme_classic() +\n",
    "     theme(text = element_text(size=16),\n",
    "           axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),\n",
    "           aspect.ratio=.75,\n",
    "           legend.position = \"bottom\",\n",
    "           legend.title = element_blank(),\n",
    "           legend.background = element_rect(fill=alpha(\"white\",0.90),\n",
    "                                            size=0, linetype=\"dotted\",\n",
    "                                            colour = \"white\"),\n",
    "           legend.text=element_text(size=12))\n",
    "\n",
    "p <- p + geom_vline(data=na.omit(test_samples_sum_stats),\n",
    "                    aes(xintercept = factor(nonov_range)),\n",
    "                    linetype=\"dashed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd8f7450-981f-4478-9752-3ba217ec07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i prop_nonov_range\n",
    "\n",
    "rownames(prop_nonov_range) <- prop_nonov_range$language\n",
    "\n",
    "# deu_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"German\")\n",
    "# eng_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"English\")\n",
    "est_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Estonian\")\n",
    "fas_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Persian\")\n",
    "# fra_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"French\")\n",
    "hrv_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Croatian\")\n",
    "jpn_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Japanese\")\n",
    "kor_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Korean\")\n",
    "nor_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Norwegian\")\n",
    "pol_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Polish\")\n",
    "por_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Portuguese\")\n",
    "# spa_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Spanish\")\n",
    "swe_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Swedish\")\n",
    "zho_prop_label <- data.frame(value=c(1000),variable = c(800),language=\"Mandarin\")\n",
    "\n",
    "p <- p + #geom_text(data = deu_prop_label,label = prop_nonov_range[\"German\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         #geom_text(data = eng_prop_label,label = prop_nonov_range[\"English\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = est_prop_label,label = prop_nonov_range[\"Estonian\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = fas_prop_label,label = prop_nonov_range[\"Persian\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         #geom_text(data = fra_prop_label,label = prop_nonov_range[\"French\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = hrv_prop_label,label = prop_nonov_range[\"Croatian\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = jpn_prop_label,label = prop_nonov_range[\"Japanese\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = kor_prop_label,label = prop_nonov_range[\"Korean\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = nor_prop_label,label = prop_nonov_range[\"Norwegian\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = pol_prop_label,label = prop_nonov_range[\"Polish\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = por_prop_label,label = prop_nonov_range[\"Portuguese\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         #geom_text(data = spa_prop_label,label = prop_nonov_range[\"Spanish\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = swe_prop_label,label = prop_nonov_range[\"Swedish\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\") +\n",
    "         geom_text(data = zho_prop_label,label = prop_nonov_range[\"Mandarin\", \"prop_samp_to_nonov\"],size=4,color=\"black\",fontface = \"bold\")\n",
    "\n",
    "#      ggsave(\"../figures/TTR_curves.pdf\", width = 8.27, height = 11.69)\n",
    "     ggsave(\"../figures/TTR_curves_wide.pdf\", width = 11.69, height = 8.27)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-focus",
   "metadata": {},
   "source": [
    "\\+ sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "collect-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i test_samples_sum_stats\n",
    "\n",
    "deu_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"German\")\n",
    "eng_n_label <- data.frame(value=c(1400),variable = c(800),language=\"English\")\n",
    "est_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Estonian\")\n",
    "fas_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Persian\")\n",
    "fra_n_label <- data.frame(value=c(1400),variable = c(1000),language=\"French\")\n",
    "hrv_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Croatian\")\n",
    "jpn_n_label <- data.frame(value=c(1400),variable = c(1000),language=\"Japanese\")\n",
    "kor_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Korean\")\n",
    "nor_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Norwegian\")\n",
    "pol_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Polish\")\n",
    "por_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Portuguese\")\n",
    "spa_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Spanish\")\n",
    "swe_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Swedish\")\n",
    "zho_n_label <- data.frame(value=c(1400),variable = c(1100),language=\"Mandarin\")\n",
    "\n",
    "deu_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"German\")\n",
    "eng_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"English\")\n",
    "est_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Estonian\")\n",
    "fas_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Persian\")\n",
    "fra_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"French\")\n",
    "hrv_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Croatian\")\n",
    "jpn_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Japanese\")\n",
    "kor_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Korean\")\n",
    "nor_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Norwegian\")\n",
    "pol_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Polish\")\n",
    "por_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Portuguese\")\n",
    "spa_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Spanish\")\n",
    "swe_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Swedish\")\n",
    "zho_sz_label <- data.frame(value=c(1400),variable = c(1700),language=\"Mandarin\")\n",
    "\n",
    "p <- p + geom_text(data = deu_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = eng_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = est_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = fas_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = fra_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = hrv_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = jpn_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = kor_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = nor_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = pol_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = por_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = spa_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = swe_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = zho_n_label,label = \"n\",size=4,color=\"black\",fontface = \"italic\") +\n",
    "         geom_text(data = deu_sz_label,label = \" = 39\",size=4,color=\"black\") +\n",
    "         geom_text(data = eng_sz_label,label = \" = 1005\",size=4,color=\"black\") +\n",
    "         geom_text(data = est_sz_label,label = \" = 22\",size=4,color=\"black\") +\n",
    "         geom_text(data = fas_sz_label,label = \" = 12\",size=4,color=\"black\") +\n",
    "         geom_text(data = fra_sz_label,label = \" = 303\",size=4,color=\"black\") +\n",
    "         geom_text(data = hrv_sz_label,label = \" = 79\",size=4,color=\"black\") +\n",
    "         geom_text(data = jpn_sz_label,label = \" = 139\",size=4,color=\"black\") +\n",
    "         geom_text(data = kor_sz_label,label = \" = 37\",size=4,color=\"black\") +\n",
    "         geom_text(data = nor_sz_label,label = \" = 56\",size=4,color=\"black\") +\n",
    "         geom_text(data = pol_sz_label,label = \" = 1\",size=4,color=\"black\") +\n",
    "         geom_text(data = por_sz_label,label = \" = 24\",size=4,color=\"black\") +\n",
    "         geom_text(data = spa_sz_label,label = \" = 31\",size=4,color=\"black\") +\n",
    "         geom_text(data = swe_sz_label,label = \" = 16\",size=4,color=\"black\") +\n",
    "         geom_text(data = zho_sz_label,label = \" = 2\",size=4,color=\"black\")\n",
    "#      ggsave(\"../figures/TTR_curves_n.pdf\", width = 8.27, height = 11.69)\n",
    "         ggsave(\"../figures/TTR_curves_n_wide.pdf\", width = 11.69, height = 8.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-attachment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
